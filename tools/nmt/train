#!/usr/bin/env bash

#=== TRAINING TASK SPECIFICATION ===

corpus_name=alice # Name of the folder containing the parallel corpus, found under $data_dir
lan_src=zh # Two-letter source language code
lan_tgt=en # Two-letter target language code
char_level=false # If set to "true", perform character-level processing (transliteration instead of translation)

#==== TRAINING PARAMETERS ====

vocsize_src=267 # Vocabulary size for the source language
vocsize_tgt=250 # Vocabulary size for the target language
edim=20 # Dimensionality of embedding layers
dim=20 # Dimensionality of hidden layers
seqlen=50 # Maximum sequence length
timeStop=60*8*1 # Maximum number of minutes to run

#==== DIRECTORY AND FILE NAMES ====

ghog_dir=$GROUNDHOG_HOME # Directory for the GroundHog Python package
nmt_dir=$NMT_HOME # Directory for the NMT Python package
#prep_dir=$nmt_dir/preprocess
prep_dir="$NMM_HOME/preprocess" # Directory for the preprocessing Python scripts
data_dir=$ghog_dir/data # Directory for the training data sets
work_dir=$data_dir/$corpus_name # Directory where the parallel corpus for this training is located
datetime=`date +%Y-%m-%d@%H-%M-%S`
model_dir=$work_dir/$datetime
this_dir=`dirname $0`
logfile=$model_dir/log # File name of the log file
configfile=$model_dir/config # File name of the config file

#==== SUBROUTINES ====

tokenize() {
	local lan=$1
	local input=$2
	local output=$3
	if [ ! -f "$output" ]; then
		perl $nmt_dir/web-demo/tokenizer.perl -a -l $lan -threads 4 < $input > $output 2>>$logfile
	else
		echo "Tokenized file \"$output\" already exists, reusing..." >>$logfile
	fi
}

numberize() {
	local input=$1
	local vocsize=$2
	local out_voc=$3
	local out_ivoc=$4
	local out_h5=$5
	if [ ! -f "$out_voc" ]; then
		local charflag=""
		if [ "$char_level" = "true" ]; then
			charflag="--char"
		fi
		python $prep_dir/preprocess.py -p $charflag -d $out_voc -v $vocsize -b $input.bin.pkl $input >>$logfile 2>>$logfile
	else
		echo "Vocabulary file \"$out_voc\" already exists, reusing..." >>$logfile
	fi
	if [ ! -f "$out_ivoc" ]; then
		python $prep_dir/invert-dict.py $out_voc $out_ivoc >>$logfile 2>>$logfile
	else
		echo "Inverse vocabulary file \"$out_ivoc\" already exists, reusing..." >>$logfile
	fi
	if [ ! -f "$out_h5" ]; then
		python $prep_dir/convert-pkl2hdf5.py $input.bin.pkl $out_h5 >>$logfile 2>>$logfile
	else
		echo "HDF5-format numberized file \"$out_h5\" already exists, reusing..." >>$logfile
	fi
}

shuffle() {
	local in_src=$1
	local in_tgt=$2
	local out_src=$3
	local out_tgt=$4
	if [ ! -f "$out_src" -o ! -f "$out_tgt" ]; then
		python $prep_dir/shuffle-hdf5.py $in_src $in_tgt $out_src $out_tgt >>$logfile 2>>$logfile
	else
		echo "Shuffled HDF5-format numberized files \"$out_src\" and \"$out_tgt\" already exists, reusing..." >>$logfile
	fi
}

train() {
	local in_src=$1
	local in_tgt=$2
	local in_voc_src=$3
	local in_voc_tgt=$4
	local in_ivoc_src=$5
	local in_ivoc_tgt=$6
	local state_file=$model_dir/train_state.py
	cat > $state_file <<- _EOF_
		dict(
		    n_sym_source=$vocsize_src,
		    n_sym_target=$vocsize_tgt,
		    rank_n_approx=$edim,
		    dim=$dim,
		    seqlen=$seqlen,
		    timeStop=$timeStop,
		    source=["$in_src"],
		    target=["$in_tgt"],
		    word_indx="$in_voc_src",
		    word_indx_trgt="$in_voc_tgt",
		    indx_word="$in_ivoc_src",
		    indx_word_target="$in_ivoc_tgt",
		    null_sym_source=0,
		    null_sym_target=0)
	_EOF_
	THEANO_FLAGS='floatX=float32' python $nmt_dir/train.py --proto=prototype_search_state --state=$state_file >>$logfile 2>>$logfile
}

write_config() {
	echo "#!/usr/bin/env bash"
	echo "### NMT-for-Mere-Mortals lab Configuration ###"
	echo "# Automatically generated by 'train'"
	echo "# Time: $datetime"
	echo
	echo "#=== TRAINING TASK SPECIFICATION ==="
	echo
	echo "corpus_name=$corpus_name # Name of the folder containing the parallel corpus, found under \$data_dir"
	echo "lan_src=$lan_src # Two-letter source language code"
	echo "lan_tgt=$lan_tgt # Two-letter target language code"
	echo "char_level=$char_level # If set to \"true\", perform character-level processing (transliteration instead of translation)"
	echo
	echo "#==== TRAINING PARAMETERS ===="
	echo
	echo "vocsize_src=$vocsize_src # Vocabulary size for the source language"
	echo "vocsize_tgt=$vocsize_tgt # Vocabulary size for the target language"
	echo "edim=$edim # Dimensionality of embedding layers"
	echo "dim=$dim # Dimensionality of hidden layers"
	echo "seqlen=$seqlen # Maximum sequence length"
	echo "timeStop=$timeStop # Maximum number of minutes to run"
}

move_files_to_model_dir() {
	mv "$this_dir/search_state.pkl" "$this_dir/search_model.npz" "$this_dir/search_timing.npz" "$model_dir"
	mv "$work_dir"/{$lan_src,$lan_tgt}.voc.pkl.{i2w,w2i} "$model_dir"
}

#==== MAIN ====

if [ ! -d $model_dir ]; then
	mkdir $model_dir
fi

# 1. Tokenize input files
echo "===Tokenizing..." >> $logfile
input_src=$work_dir/$lan_src
input_tgt=$work_dir/$lan_tgt
tok_src=""
tok_tgt=""
if [ "$char_level" = "true" ]; then
	tok_src=$input_src
	tok_tgt=$input_tgt
else
	tok_src=$work_dir/$lan_src.tok
	tok_tgt=$work_dir/$lan_tgt.tok
	tokenize $lan_src $input_src $tok_src
	tokenize $lan_tgt $input_tgt $tok_tgt
fi

# 2. Preprocess
echo "===Preprocessing..." >> $logfile
voc_src=$work_dir/$lan_src.voc.pkl
voc_tgt=$work_dir/$lan_tgt.voc.pkl
ivoc_src=$work_dir/$lan_src.ivoc.pkl
ivoc_tgt=$work_dir/$lan_tgt.ivoc.pkl
h5_src=$work_dir/$lan_src.h5
h5_tgt=$work_dir/$lan_tgt.h5
in_train_src=$work_dir/$lan_src.shuf.h5
in_train_tgt=$work_dir/$lan_tgt.shuf.h5
numberize $tok_src $vocsize_src $voc_src $ivoc_src $h5_src
numberize $tok_tgt $vocsize_tgt $voc_tgt $ivoc_tgt $h5_tgt
shuffle $h5_src $h5_tgt $in_train_src $in_train_tgt

# 3. Train
echo "===Training..." >> $logfile
train $in_train_src $in_train_tgt $voc_src $voc_tgt $ivoc_src $ivoc_tgt

# 4. Final works
write_config > $configfile
move_files_to_model_dir
